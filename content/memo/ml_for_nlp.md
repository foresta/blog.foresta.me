+++
date = "2018-06-02T12:21:54+09:00"
title = "自然言語処理のための機械学習入門"
thumbnail = ""
tags = ["自然言語処理", "機械学習"]
categories = ["メモ"]
draft=true
+++


# 自然言語処理のための機械学習入門

## 自然言語処理外観
主なタスク

#### 構造解析

- 単語分割
- 品詞タグ付け
- 構文解析

#### 目的としてのタスク

- 文書分類
- 照応解析
- 質問応答
- 機械翻訳

`コーパス` -> 数式 -> 機械学習

## 数学知識
### 最適化問題

解析的に解ける問題と解けない問題がある. \
前者は閉形式と呼ばれるが解析的に解ける問題は実際は少ない \
=> アルゴリズムによって近似していく \
=> 最適化問題

中でも `凸計画問題` は解きやすい.(この形に持っていければハッピー)

#### 凸計画問題
ある最適化問題が凸計画問題である
=> 目的関数が `凸関数` かつ実行可能領域が `凸集合` である

##### 凸集合
集合内の任意の2点を結ぶ線分が集合からはみ出ない \
$$
x^{(1)}, x^{(2)} \in A → tx^{(1)} + (1-t)x^{(2)} \in A
$$

##### 凸関数
１次条件: 上に凸ならすべての接戦が関数の上側 \
２次条件: 　$f"(x) \ge 0$

##### 凸関数の証明

凸集合であるか？ \
関数上の2点を結ぶ線分が関数の内側にある(上に凸なら必ず下側にある) \
$$
f(tx1 + (tー1)x2) \ge tf(x\_1) + (t-1)f(x\_2)
$$

###### 凸関数の1次条件
接線が関数の外側にあるか？(上に凸なら必ず上側にある)

$$
f(x^{(2)}) - f(x^{(1)}) <= \frac{\partial f(x^{(1)})}{\partial x}(x^{(2)} - x^{(1)})
$$

(ヒント) \
$傾き = \frac{高さ}{幅}$ \
$高さ = 傾き \times 幅$ \
$ \frac{\partial f(x^{(1)})}{\partial x} = 傾き$

###### 凸関数の2次条件


#### ラグランジュ乗数法

### 確率論
#### 離散確率

### 情報理論
#### エントロピー
#### KLダイバージェンス
#### ジェンセン・シャノン・ダイバージェンス
#### 自己相互情報量


## 文字及び単語の数学的表現
#### タイプとトークン
#### nグラム
#### データスパースネス問題
#### 確率分布を用いた文書・単語の表現

## クラスタリング
#### 凝集型クラスタリング
#### k-means
#### EMアルゴリズム
- 混合正規分布によるクラスタリング
- PLSA(確率的潜在意味解析)

## 分類
#### ナイーブベイズ分類器
#### SVMとカーネル法
#### 対数線形モデル
- ロジスティック回
- 最大エントロピーモデル

## 系列ラベリング
#### 系列ラベリング問題とは？
#### 隠れマルコフモデル
#### 分類器による系列ラベリング
#### 条件付き確率場
#### チャンキング

## 実験の仕方など
#### データやプログラムの入手法
#### データセットの取り扱い
#### 評価
#### 検定法

