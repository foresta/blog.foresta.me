+++
date = "2018-06-02T12:21:54+09:00"
title = "データサイエンスのための統計学入門"
thumbnail = ""
tags = ["統計", "データサイエンス", "機械学習"]
categories = ["メモ"]
draft=true
+++

# 1. 探索的データ分析

## 1.1 データ型

データ型を意識すると統計的手続きを適切に指示しやすい

* 連続データ ex) 気温、株価
* 離散データ ex) 出現回数
* カテゴリデータ ex) 動物の種類
* バイナリデータ ex) True or False
* 順序尺度データ ex) 順序付き 1,2,3,4,5

## 1.2 矩形データ

データフレーム: スプレッドシートのような表データ
特徴量: 表のカラム(列)
    属性、入力、予測変数、変数
成果(変数): 予測の結果、目標
    従属変数、応答、目標、出力
レコード: 表の行(1サンプルデータ)
    ケース、事例、インスタンス、観測、パターン、サンプル

データフレームにはインデックスがついてる.
    pythonのpandasライブラリなど参考

### 非矩形データ

時系列データ: 同じ変数を継続的に測定したレコード
空間データ: 地図、位置分析など
オブジェクト: 
グラフ(ネットワーク)データ構造: 抽象的関係性を表現 ex)ソーシャルネットワーク
    ネットワーク最適化やレコメンデーションシステムなど。

※ 本書では矩形データのみ扱う

## 1.3 位置推定

探索的データ解析: EDA(Exploratory Data Analysis)

各特徴量の「代表値」（ほとんどのデータが位置する推定値）を求めることからスタート

位置推定

* 平均値(mean): 総和を数で割ったもの
* 加重平均(weighted mean): 値に重みをかけた平均
* 中央値(median): その値の上下に半分ずつデータが位置するような値
* 加重中央値(weighted median): 重みをかけた中央値
* トリム平均: 外れ値を除外後の平均
* 頑健性(robustness): 異常値に影響されないこと、ロバスト性
* 外れ値: データから大きく外れたデータ

### 平均値
#### 平均

$$
平均 = \frac{全データの総和}{データ数} \
$$
$$
\overline{x} = \frac{\sum^{n}\_{i=1}x\_{i}}{n}
$$

```
mean(data)
```

#### トリム平均

異常値を取り除いた平均. \
外れ値の影響を受けないようにするために利用. \
もっとも小さい値からp個, もっとも大きい値からp個取り除いたデータ集合をxを整列したものを \
$x\_{(i)}: x\_{(1)}, x\_{(2)}, x\_{(3)}, ...,  x\_{(n)}$ \
で表すと

$$
トリム平均 = \frac{取り除いた後のデータの総和}{取り除いた後のデータ数}
$$
$$
\overline{x} = \frac{\sum^{n-p}\_{i=p+1}x\_{(i)}}{n-2p}
$$

```
mean(data, trim=0.1)
```

#### 加重平均
重みのデータ$w\_{i}$を用いて

$$
加重平均 = \frac{データ×重みの総和}{重みの総和}
$$

$$
\overline{x\_{w}} = \frac{\sum^{n}\_{i=1}w\_{i}x\_{i}}{\sum^{n}\_{i=1}w\_{i}} 
$$

```
library('matrixStats')
weightedMedian(data, w=weighted)
```

### 中央値と頑健推定

代表的な指標は平均値だが、外れ値の影響を受けやすい.
中央値, トリム平均はロバスト.

## 1.4 散らばりの推定

偏差
: 観測値を位置推定値の差
: 誤差、残差

分散
: 平均からの偏差の二乗和をn-1で割ったもの
: 平均二乗誤差

標準偏差
: 分散の平方根
: L2ノルム, ユークリッド距離

平均絶対偏差
: 平均から偏差の絶対値の平均
: L1ノルム, マンハッタン距離

中央値絶対偏差
: 中央値からの偏差の絶対の中央値

範囲
: データセットの最大値の最小値の差

順序統計量
: 最小から最大へと整列したデータ値に基づく統計量
: 順位

パーセンタイル
: P%がこの値以下、100-P%が以上になる時の値
: 分位数

四分位範囲
: 75パーセンタイルと25パーセンタイルの差
: IQR

### 標準偏差と関連推定値

散らばりを測る->偏差を求める

#### 平均絶対偏差

平均値からの偏差(各データの差分)の平均

$$
平均絶対偏差 = \frac{\sum^{n}\_{i=1}|x\_{i}-\overline{x}|}{n}
$$

#### 分散と標準偏差

分散:偏差の二乗和をn-1で割ったもの \
n-1なのは、自由度という概念が関係するらしい. \
標準偏差: 分散の平方根

$$
分散 = s^2 = \frac{\sum^{n}\_{i=1}(x\_i-\overline{x})^2}{n-1}
$$
$$
標準偏差 = s = \sqrt{分散}
$$

```
sd(data) # 標準偏差
```

#### 中央値絶対偏差
(MAD: Median Absolute Deviation fomr in the median)
異常値に影響されない=ロバスト性

m = 全データの中央値として
$$
MAD = 中央値(|x\_1-m|,|x\_2-m|,...,|x\_n-m|)
$$

```
mad(data)
```

### パーセンタイルに基づく推定値

順序統計量の推定値

2つのパーセンタイル間の値の差を範囲とする。

よく使われるのは四分位範囲(IQR: interquartile range)
例）データ: 1,2,3,3,5,6,7,9
25パーセンタイル = 2.5 (2と3の間に区切り)
75パーセンタイル = 6.5（6と7の間に区切り）

$$
IQR = 6.5 - 2.5 = 4
$$

```
IQR(data)
```

## 1.5 データ分布の探索

### パーセンタイルと箱ひげ図
データ: パーセンタイル
プロット: 箱ひげ図

パーセンタイルに基づき、
下25パーセンタイル,上75パーセンタイルでの箱

箱の中の水平線が中央値、データの大まかな範囲を表すための点線(ヒゲ)

```
# パーセンタイル
quantile(data, p=c(.05, .25, .5, .75, .95))

# 箱ひげ図
boxplot(data / numOfData, ylab="y label")
```

### 度数分布表とヒストグラム
データ: 度数分布表
プロット: ヒストグラム

度数分布表
: 等間隔でデータを区切りその個数を表した表

ヒストグラム
: それのグラフ

調査: 統計モーメント

```
# 度数分布表
breaks <- seq(from=min(state[["Population"]]), to=max(state[["Population"]]), length=11)
pop_freq <- cut(state[["Population"]], breaks=breaks, right=TRUE, include.lowest = TRUE)
table(pop_freq)

# ヒストグラム
hist(data, breaks=breaks)
```
### 密度推定
データ: カーネル密度推定
プロット: 密度プロット

カーネル密度推定を用いてデータの分布の密度を表した図

要調査: カーネル密度推定

```
density(data) # 密度推定

# 線でプロット
lines(density(data), lwd=LineWiDth, col=COLor)
```

## 1.6 バイナリデータとカテゴリデータの探索
プロット: 棒グラフ, 円グラフ

### 最頻値(モード)

データでもっとも頻度の高い値(複数可)

### 期待値


$カテゴリデータ: {d1, d2, d3, ...}$ \
$確率: {p1, p2, p3, ...}$

$$
EV = p1 * d1 + p2 * d2 + p3 * d3 ...
$$

## 1.7 相関

相関係数
: 数値同士が互いに関連数程度(-1 ~ +1)

相関行列
: 行と列が変数、セル値が変数間の相関係数

散布図
: x軸に変数値, y軸にもう一つの変数値のグラフ

#### ピアソンの相関係数

変数: {x, y}
xの平均からの偏差とyの平均からの偏差をかけて、それを標準偏差の積で割る \
nではなくn-1で割る(自由度)

* +1: 完全な正の相関
* -1: 完全な負の相関
*  0: 相関なし

$$
r = \frac{\sum^{n}\_{i=1}(x\_i - \overline{x})(y\_i - \overline{y})}{(n-1)s\_{x}s\_{y}}
$$

※相関係数は外れ値の影響を受ける。

要調査: ロバストな相関係数（順位に基づく）

* スピアマンの$\rho$
* ケンドールの$\tau$

```
# 相関行列のグラフ
library(corrplot)
corrplot(data, method="ellipse")
```

```
# robustなRパッケージ
robust
covRob
```

### 散布図

相関を可視化するのに役立つ。よく見るやつ

```
plot(x, y)
```

## 1.8 二つ以上の変量の探索

平均や分散 => 一変量解析
相関分析   => 二変量解析
それ以上のやつ => 多変量解析

多変量解析

分割表
: 2つ以上のカテゴリ変数のカウントをまとめた表

六角ビニングプロット
: 六角形のビンで描いた二つの数値変数のレコードのプロット(ヘキサゴナルビニング)

等高線プロット
: 地図の等高線のようなやつ. ２変数と密度. (コンター図, コンタープロット)

バイオリンプロット
: 箱ひげ図と同じようなやつ、密度推定を示す

### 六角ビニングと等高線(二つの数値データのプロット)

散布図ではわかりにくい密度を見やすくしたような図

```
# 六角ビニング
nrow(data)

# 等高線
ggplot(kc_tax0, aes(SqFtTotLiving, TaxAssessedValue)) +
theme_bw() +
geom_point( alpha=0.1) +
geom_density2d(colour="white") +
labs(x="Finished Square Feet", y="Tax Assessed Value")

```


### 二つのカテゴリ変数の探索

2カテゴリ変数の要約に使う

### カテゴリデータと数量データ

バイオリンプロットは箱ひげ図に密度推定をプロットしたようなもの
分布のニュアンスをつかむのに良い

```
# バイオリンプロット
ggplot(data=airline_stats, aes(airline, pct_carrier_delay)) +
ylim(0, 50) +
geom_violin() +
labs(x="", y="Daily % of Delayed Flights")
```

### 多変量の可視化

条件分けによって、より多くの変数に拡張できる.

```
ggplot
```

## 1.9 まとめ

データを見たらまず、可視化してみるところをやる。
→探索的データ分析(EDA)

# 2. データと標本の分布

母集団
: 統計的に未知の分布に従うとされる。

標本データ
: 実際に手に入れられたデータ


伝統的な統計学は母集団の理論、現代統計学は標本に焦点がある。(強い仮定を必要としない)
データサイエンティストは、標本抽出の手続きと手元のデータの焦点を絞る。

## 2.1 無作為抽出と標本バイアス
標本
: 大きなデータセットの部分集合

母集団
: 起きなデータセット(または仮想的なデータセット)

N(n)
: 母集団(標本)の大きさ

無作為抽出(ランダムサンプリング)
: ランダムに選ぶ

層化抽出
: 母集団をそう別にグループ分けして、各層から無作為抽出する

単純無作為標本
: 無作為抽出して標本データ

標本バイアス
: 母集団（の性質・分布）を正確に反映できていない標本

復元抽出
: 抽出済みのデータを再度抽出の対象となるように母集団に戻す

非復元抽出
: 戻さない


データの品質
* 完全性
* フォーマットの一貫性
* 清浄度
* 個別データポイントの正確さ
* 代表性

自己選択標本バイアス
レビューなど。書こうという意欲がある人。バイアスがある。

### バイアス

サンプリング(標本化)した時のデータの偏り

### 無作為抽出

母集団の適切な定義が鍵になる。
例）顧客アンケート

まず顧客の定義。
購入金額>0のすべての顧客の記録なのか、返金した顧客は含めるのか？など

標本抽出の手続き
データが時間によって性質が変わる場合に、タイミングはいつが良いのか？

層別抽出は、母集団をグループ分け。
各層で、無作為抽出標本を取り出す。
各層の標本データを重み加算して、サンプルサイズが等しくなるようにする。

### サイズと品質

データサイズが大きすぎると、データ探索(可視化)が大変だし、データ品質は低下する。
無作為抽出をすると、バイアスを減らしたり、上記2点が改善したりする

ビッグデータの価値は、データが大きいだけでなく、「疎」である場合。
自然言語処理とかは疎なデータになりやすい。

### 標本平均と母集団平均

$\overline{x}$
: 標本平均 => 観測可能

$\mu$
: 母集団の平均 => 推論するしかない

ビッグデータ時代でも、無作為抽出は大事。
データの品質はデータ量よりも重要なことが多い
無作為抽出はバイアスをなくす

## 2.2 選択バイアス

バイアス
: 一定の傾向を持った系統誤差

データスヌーピング
: 何か興味深いものを求めて、データを探索すること

膨大探索効果
: 大量の予測変数からデータをモデル化して得られるバイアスまたは再現不能な結果

データのパターンが本物なのか？データスヌーピングの結果に過ぎないのか？

> 十分時間をかけてデータを拷問すれば、いずれは白状する

選択バイアスは注意を払う必要がある
データ探索の末見つけた発見が、本当に興味深いものなのか？はたまた偶然による外れ値なのか？

要調査: 並べ替え検定


### 平均への回帰

測定結果は回数が多くなるほど、中心に近い観測地が多くなる。

* 仮設設定 -> ランダム化と無作為抽出原則に基づきデータ収集
* データ分析は、データ収集/分析プロセスの結果によるバイアスの危険性が伴う

## 2.3 統計量の標本分布

標本統計量
: 大きな母集団から抽出した標本データで計算された統計量

データ分布
: データ個々の値の度数分布

標本分布
: 多数の標本もしくは、リサンプリングした標本の標本統計量

中心極限定理
: サンプルサイズが大きくなればなるほど、標本分布が正規分布の形に近く傾向

標準誤差
: 多数の標本における標本統計量の標準誤差のこと(ここのデータの標準誤差とは別物)

### 中心極限定理

たとえ母集団が正規分布に従っていなくても \
サンプルサイズが十分大きく、データ分布が正規分布からひどくかけ離れてなければ、\
複数の標本の平均が正規分布にちかづく。

要調査: 仮設検定、信頼区間


### 標準誤差

標本値の標準誤差 $s$ とサンプルサイズ $n$

$$
SE(Standard Error) = \frac{s}{\sqrt{n}}
$$

サンプルサイズを増やすほど、SEは減る。
SEを半分にするには、サンプルサイズを4倍にする必要がある。

統計量: 平均とかそういう標本（データセット）から算出された値

標本ごとに統計量の度数分布をみると、標本ごとに統計量がどのくらい違うか確かめることができる \
標準誤差は標本統計量の散らばりを要約する基本的な統計量


標本分布はブートストラップ、中止極限定理による公式で推定できる。

## 2.4 ブートストラップ

ある標本から、追加の標本を復元抽出する。
概念的に、元の標本を何千あるいは、何万と複製して、元の標本のすべての知識を具現化した母集団を仮想的に作るようなもの。この仮想的な母集団から、標本分布の推定する目的で標本を抽出

ブートストラップ標本
: 観測されたデータセットから復元抽出された標本

リサンプリング
: 観測されたデータから繰り返し、標本抽出する手続き。ブートストラップ＋置換(シャッフル)

平均値のブートストラップ・リサンプリングのアルゴリズム

1. 標本の値を抽出し、記録し、戻す(リサンプリング)
2. n回繰り返す
3. n個のリサンプリング値の平均を記録
4. 上の1から3のステップをR回繰り返す
5. R個の結果を使って次を行う

```
    a. 標準偏差の計算（標本平均標準誤差の推定
    b. ヒストグラム、または箱ひげ図を作る
    c. 信頼区間を計算する
```

```
library(boot)
stat_fun <- function(x, idx) median(x[idx])
boot_obj  <- boot(loans_income, R = 1000, statistic=stat_fun)
```

バギング（baggin, 「bootstrap aggregating」）と6.3参照

### リサンプリングとブートストラップ

リサンプリングは並べ替え検定(3.3.31)を指すことが置い。
ブートストラップは、観測データセットからの復元抽出を意味する

ブートストラップは、標本統計量の変動性を評価できる。
複数のブートストラップ標本予測の集約（バギング）は、一つのモデルより性能が良い

要調査: バギング

## 2.5 信頼区間

標本推定における潜在的な誤差を見つける手法。

信頼水準
: 信頼区間が、対象統計量を含むと期待される％

サイズnの標本でのブートストラップ信頼区間のアルゴリズム

1. データからサイズがnの無作為標本を復元抽出する（リサンプリング）
2. このリサンプリングの対象統計量(平均値とか)を記録
3. 1と2をR回繰り返す
4. x%信頼区間のために、分布の両端でR個リサンプリング結果から$[(1-\frac{x}{100}) / 2]$%を切り捨てる
5. 切り捨て点がx%ブートストラップ信頼区間の端点

xを信頼水準とか呼ぶ

## 2.6 正規分布

誤差
: データポイントと予測値または平均値との差

標準化
: 平均値を引いて標準偏差で割る

z値
: ここのデータを標準化した結果の値

標準正規分布
: ここのデータを標準化した結果の値

### 標準正規分布とQQプロット

標QQプロット
: 標本分布がどの程度正規分布に近いかを可視化するプロット

x軸を正規分布に対応する分位数, y軸にz値を表す
正規分布に近い = プロットした結果が対角線上(y = x)にある

```
norm_samp <- rnorm(100)
qqnorm(norm_samp)
abline(...)
```

## 2.7 ロングテールの分布

正規分布に従っていない
正規分布に従うと仮定してしまうと、極端な事象を過小評価する

QQプロットをすると、z値が低い値はより低く、高い値がより高くなる傾向にある。

株式市場などで発生するブラックスワン理論はまさにこれ。

## 2.8 スチューデントのt分布

t分布は正規分布の裾が少し暑くて長い。
標本統計量の分布は通常t分布の形になる。
(標本の大きさが大きいほど、正規分布に近く)

t分布と比較することで、標本変動性に関して信頼区間を推定できる。

サイズnの標本で平均値$\overline{x}$を計算し、sをこの標本の標準偏差とする.
表保平均を中心とした信頼区間は以下の式で計算できる

$$
\overline{x} \pm t\_{n-1}(0.05) \times \frac{s}{n}
$$


$t\_{n-1}(0.05)$は自由度(n-1)のt統計量の値。t分布の両端の5%を切り捨てる。
この場合、90%の信頼区間と言える？

## 2.9 二項分布

コインの表裏などの答えが2つになる事象の分布
ベルヌーイ分布

データ分析の核心(買うor買わない、クリックするorしない etc...)

二項確率
```
# 各試行の成功確率p=0.1, 試行回数はsize=5, x=2回成功する時の確率の計算
dbinom(x=2, size=5, p=0.1)

# n回の試行で成功がx回以下の確率
pbinom(x=2, size=5, p=0.1)
```

平均値
: $n \times p$

分散
: $n \times p(1-p)$

## 2.10 ポワソン分布と関連する分布


### ポワソン分布

時間または空間の単位標本での発生数の度数分布
時間あたりとか空間当たりの単位標本が多数ある時の、単位ごとの事象分布.
例えば...
「サーバーに到達したインターネットトラフィックを5秒以内に完全に処理するのを95%確実にするにはどの程度のキャパシティが必要か？」

ポアソン分布のパラメータは $\lambda$

```
# lambda=2ポアソン分布で100個の乱数を生成する.
rpois(100, lambda = 2)
```

要調査: ポワソン分布


### 指数分布

Webサイトの訪問者や料金所に到着する車など。

事象間の時間分布をモデル化
故障寿命などに使われたり。

```
# 単位時間あたりの平均事象数が0.2, 100個の乱数を発生
rexp(n = 100, rate = .2)
```

### 故障率の推定

通常$\lambda$は, 既知もしくは以前のデータから推定可能なことが多い。 

しかし、ごく稀にできないこともある。
そういった場合には、これ以下の比率はありえないと言う閾値を推定することができる。
カイ二乗検定を様々なデータに適用すると、観測データとどのくらい適合するか決定できる

要調査: カイ二乗検定 


### ワイブル分布

指数分布の拡張。

事象発生率が形状パラメータ$\beta$ \
$\beta > 1$で時間とともに事象確率が増加, \
$\beta < 1$で時間とともに事象確率が現象

寿命特性$\gamma$: どういう効果かはよくわかってない。

要調査: ワイブル分布

```
形状パラメータが1.5, 寿命特性5000のワイブル分布から100個の乱数を生成
rweibull(100, 1.5, 500)
```

## 2.11 まとめ

ビッグデータ時代だが、データの推定は重要なことがある。
無作為抽出原則の重要性は高い。
データのバイアスを減らし、品質の高いデータを使うことが重要

# 3. 統計実験と優位性検定

仮設から結論、推論出すための手法。

統計的推論のパイプライン /
仮設設定 -> 実験計画 -> データ収集 -> 推論/結論

統計的推論, t検定, p値

## 3.1 A/Bテスト

処置
: 被験者が試される(薬、価格、Web見出しなど)もの

処置群
: 被験者のグループ


2グループで結果を比較する。
使われる検定統計量は、よく使われるのは２値変数。購入したかどうか？やクリックしたかどうかなど
被験者が処置を受ける \
被験者は、グループに無作為抽出されることが好ましい

### なぜ統制群があるか

統制群
: 処置を受けない、もしくは標準的処置を受ける被験者グループ

統制群があることで、検定の比較のベースラインとなる

### なぜA/BだけでC,D, ではないのか

A/Bだけが唯一の種類ではない。
複数の処置の場合には、「多腕バンディット」のような比較的新しい実験計画方が使われる。

## 3.2 仮説検定

A/Bテストなどは、仮設を念頭に作られる。\

処置AとBの差は以下のいずれかによるもの

* 被験者の無作為抽出の偶然
* AとBの真の差によるもの

つまり統計的仮説検定は検定の結果生まれたAとBの差が偶然なのかどうかを評価していく.

### 帰無仮説

「A/Bテストなどの結果の差が偶然によって起こったのだとする仮説」
この仮説を否定できれば良い。（偶然によるものより大きな差を生み出していると言うことを示す）

示すのには、並べ替えリサンプリング手続きなどがある。。


### 対立仮説

帰無仮説に対立する仮説


### 片側、両側仮説検定

#### 片側仮説

既存のAに対し、新たなBを試したとする。 \
新たなBが決定的に優れていることを仮説では仮定する。 \
そのばあ、BがAより良いの1方向の対立仮説なので、片側仮説検定

#### 両側仮説

AはBと異なる。大きいまたは小さい場合の両方向の対立仮説
(大きくても小さくても良い。両端をチェックする), 両側仮説検定

## 3.3 リサンプリング

観測データから標本値を繰り返し取得する \
目的: ランダムな変動性の評価。（バギングとランダムフォレストなど）\
リサンプリングには、並べ替え検定とブートストラップの２種類がある

### 並べ替え検定

複数の標本を組み合わせてシャッフル。

二つの以上の標本を一緒にして、ランダムに観測データを並べ替えて、リサンプリングする \
(確率化検定、無作為並べ替え検定、正確確率検定)

1. 1つのデータセット中の異なるグループの結果を一緒にまとめる
2. まとめたデータをシャッフルして、無作為抽出→グループAと同じサイズの標本をリサンプリング
3. 残りのデータから無作為抽出して、グループBと同じサイズの標本をリサンプリング
4. グループC, Dがあれば同様に行う
5. 統計量や推定ちを新しくリサンプリングして標本に対して再計算して記録する
    ここまでが並べ替え検定の1回分
6. 上記ステップをR回繰り返し検定統計量の並べ替え分布を得る

元のグループで観測された差を並べ替えを行った差の集合と比較。\
元の観測された差が並べ替え後の集合の範囲内→偶然による範囲内。
元の観測された差が並べ替え後の集合の範囲外→偶然によるものでは無いと結論できる。-> 差が統計的有意と言える

### 例：Web粘着性

webページの滞在時間を長くしたい \
→ 検定統計量はwebページのセッション時間(平均)
ページAは21セッション、ページBは15セッションで合計36セッション
ページA平均時間 - ページB平均時間 = 35.67
この差が偶然によるものなのかどうか検定する。

並べ替え検定 \
36セッションを無作為に21のグループ（A）と15のグループに分ける. \
この無作為に分けた後のグループ同士の、検定統計量(セッション時間平均)の差をとる。
この無作為グループ分けを、R=1000回とか繰り返し、差が35.67を超えるかどうかを見れば良い。
しばしば超えるようであれば、偶然によるものと考えられる。

### 完全並べ替え検定とブートストラップ並べ替え検定

無作為並べ替え検定や、確率か検定とも呼ばれるランダムにシャッフルするものの他に次の２種類のやり方がある。

* 完全並べ替え検定
* ブートストラップ並べ替え検定

完全並べ替え検定
: 分割のすべての場合について調べる. サンプルサイズが比較的小さい場合に行うことができる. 完全確率検定とも呼ばれる（検定のα水準よりも帰無モデルが「有意」になることが無いことが保障されるため. 3.4参照）

ブートストラップ並べ替え検定
: グループを抽出するときに、復元抽出が用いられる。母集団からの被験者選択もランダムであるようにモデルかできる。

### 並べ替え検定：データサイエンスの基本

ランダムな変動の果たす役割を探るための、ヒューリスティックな手続き。
テストの結果が、効果があるのか、ランダムなのかを検定するためのアプローチ.

## 3.4 統計的優位性とp値

統計的優位性
: 実験が偶然の結果とは言えない極端な結果を生み出したかどうか

### p値

帰無仮説を具現化する偶然のモデルにおいて、p値は観測結果の異常な（極端な）値が得られる確率

### アルファ

悔過が有意であるために、偶然の結果が超えなければならない「異常性」の確率の閾値
(p値のこと？)

#### p値の価値
### 第一種の過誤と第二種の過誤

第1種の過誤
: 効果が「偶然なのに」本物だと結論付けること


第2種の過誤
: 効果が「本物なのに」偶然のものだと結論付けること

### データサイエンスとp値

モデルの結果が偶然の範囲なのかどうかで役立つ測定値がp値。

## 3.5 t検定

スチューデントのt分布にちなんだt検定 \
有意性検定を行うためには、観測した結果が正規分布の範囲内かどうかを決定する必要がある。
リサンプリング検定を何千回を行うのが難しかった頃に、よい近似としてはっけんされたのが、t分布にもとづくt検定.
A/Bテストによく用いられた。 \
ただし、検定統計量の標準化を行う必要があった。

R,Pythonなどには組み込みの関数があるのでそれをつかう

```
t.test(Time ~ Page, data=session_times, alternative='less' )
```


## 3.6 多重検定

多重性（多重比較、多数の変数、多数のモデルなど）は偶然を有意と結論づけてしまうリスクを増大させる。
交差検証(4.2.3)やホールドアウト集合を用いて誤解を招く結果を排除する

## 3.7 自由度

自由に変更できる値の個数. \
例えば, 10個の値の平均がわかっていて, 9個の値を知っている
=> 残りの10番目の値がわかる.
=> 自由に変更できるのは9個

d.f.
: 自由度(degree of freedom)

## 3.8 ANOVA
### F統計量
### 二次元配置分散分析(二元ANOVA)
i
## 3.9 カイ二乗検定
### カイ二乗検定：リサンプリング方式
### カイ二乗検定：統計理論
### フィッシャーの正確確率検定
### データサイエンスへの関わり

## 3.10 多腕バンディットアルゴリズム

複数の選択肢（アーム）の表示率を動的に変えていき、より良い結果になるようにするアルゴリズム. 
WebサイトA, B, Cがあって最初は当分に表示するが、Aが良い結果となったら、Aの表示率を上げていく.

イプシロン貪欲アルゴリズム

パラメータε

1. 0~1までの範囲の乱数を生成
2. 乱数が定数εより小さいならば、同じ割合で表示(二つならば50% - 50%)
3. 数がε以上であれば, これまでの応答率が最高のものを表示

トンプソンサンプリング
: ベイズの方式, ベータ分布を使う.

A/Bテストではよくない処置をなんども繰り返すのに対し、多腕バンディットではできるだけよくない処置をしないようにしている. 3つ以上の処置を効率よく扱うのに便利

## 3.11 検定力とサンプルサイズ

どのくらいの期間, 検定(A/Bテスト)を行えば良いのか？

効果量
: 統計的検定で検出できると期待する効果の最小量.「クリック率の20%向上!」など

検定力
: 指定されたサンプルサイズで効果量を検出する確率.

有意水準
: 検定を行う統計的水準.

必要なデータ量(サンプルサイズ)はAとBの差がどのくらいあるかによる.


### サンプルサイズ

検出力計算: どのくらいの標本が必要なのかを推定したい
例) CTRを調べて新しい広告と既存の広告の差を検定する.このためにはクリック数はどのくらい必要なのか？

まずは効果量を決める. 「新しい広告が10%以上の効果があれば新しい方を使う」\
CTR 1.1% -> 1.21% にするとする.

1.1%の1の箱A(110個の1と9890個の0)と1.21%の箱を用意する。\
そこから300回抽出する(300人がクリックするかどうか？)

結果 \
A: 3つの1 \
B: 5つの1

結果の5:3は偶然の範囲内っぽい.
n = 300は10%の効果量を測るには小さすぎた.(信頼性に欠ける)

そこで、サンプルサイズをn=2000, 効果量を30%にしてみる

CTR 1.1% -> 1.65% \
2000回抽出

結果 \
A: 19
B: 34

ちょっと有意っぽい？


検定力や必要なサンプルサイズを求めるのは、4変数が必要

* サンプルサイズ(n = 2000)
* 検出したい効果量(CTR 10% UP)
* 検定を行う有意水準(α: どの程度なら有意な結果か？)
* 検定力(どのくらいの確率で効果量を検出できるか?)


## 3.12 まとめ


検定のやり方.

統制群を含める.
仮設検定, p値, t検定.

本当に効果があるのか？を確かめるのが検定

# 4. 回帰と予測

X(ある変数)とY(ある変数,結果)の関係式を求める \
XからYを予測する

## 4.1 単回帰
### 回帰式

直線のモデルで予測する


$$
Y = b\_0 + b\_1X
$$

予測変数をExposure, 応答をPEFRとすると

$$
PEFR = b\_0 + b\_1 Exposure
$$

R
```
# lm: lenear model
model <- lm(PERF ~ Exposure, data=lung)
```

### 予測値と誤差

R

```
# predict
fitted <- predict(model)

# error
resid <- residuals(model)
```

### 最小二乗法

RSS: residual sum of squares(残差平方)

$$
RSS = \sum^{n}\_{i=1}(\hat{Y} - Y\_i)^2
$$

### 予測と説明（プロファイリング）

もともと回帰は、あるXとYの関係を説明するためのもの.
現在では、新しいものの予測にも使われる

## 4.2 重回帰

$$
Y = b\_0 + b\_1X\_1 + b\_2X\_2 + ... + b\_pX\_p + e
$$

### モデルの評価

RMSE: (Root Mean Squared Error), 平均二乗誤差平方根

$$
RMSE = \sqrt{\frac{\sum^{n}\_{i=1}(y\_i - \overline{y\_i})^2}{n}}
$$

RSE: Root Standard Error??? \
分母が自由度

$$
RSE = \sqrt{\frac{\sum^{n}\_{i=1}(y\_i - \overline{y\_i})^2}{n - p - 1}}
$$

決定係数, R二乗統計量, $R^2$

分子はハットで予測値, 分母はバーで観測値

$$
R^2 = 1 - \frac{\sum\_{i=1}^{n}(y\_i = \hat{y\_i})^2}{\sum\_{i=1}^{n}(y\_i = \overline{y\_i})}
$$

t統計量

SE: 標準誤差

$$
t\_b = \frac{\hat{b}}{SE(\hat{b})}
$$

### 交差検証

k分割交差検証

1. データをk個に分ける
2. 1/kを検証に使い k-1/kで訓練
3. すべてのパターンでやる
4. 平均化、モデルの評価指標と組み合わせる


### モデル選択と段階的回帰

できるだけシンプルなモデルを選択する.

変数を追加する=RMSEが減り、$R^2$が増える

AICと言うモデル選択の指標がある.

Pが変数の個数, nはレコード数

$$
AIC = 2P + n log(RSS/n)
$$

適切なモデルを選択する方法

* 段階的回帰
* 罰則付き回帰


### 加重回帰

## 4.3 回帰を使った予測
### 外挿の危険性
### 信頼区間と予測区間

## 4.4 回帰でのファクタ変数
### ダミー変数表現
### 多水準のファクタ変数
### 順序ファクタ変数

## 4.5 回帰式の解釈
### 相関予測変数
### 多重共線性
### 交絡変数
### 交互作用と主効果

## 4.6 仮定をテストする：回帰診断
### 外れ値
### 影響値
### 不等分散性、非正規性、相関誤差
### 偏残差プロットと非線形性

## 4.7 多項式回帰およびスプライン回帰
### 多項式
### スプライン
### 一般化加法モデル

## 4.8 まとめ

# 5. 分類

## 5.1 ナイーブベイズ
### 正確なベイズ分類はなぜ実用的でないか
### ナイーブベイズ解
### 数値予測変数

## 5.2 判別分析
### 共分散行列
### フィシャーの線形判別
### 簡単な例

## 5.3 ロジスティック回帰
### ロジスティック応答関数とロジット
### ロジスティック回帰と一般化線形モデル
### 一般化線形モデル
### ロジスティック回帰の予測値
### 係数とオッズ比を解釈する
### 線形回帰とロジスティック回帰：類似点と相違点
### モデルを評価する

## 5.4 分類モデルの評価
### 混同行列
### 稀なクラスの問題
### 適合率、再現率、特異度
### ROC曲線
### AUC
### リフト

## 5.5 不均衡データの戦略
### アンダーサンプリング
### オーバーサンプリングと重み追加/削減
### データ生成
### コストベース分類
### 予測を探索する

## 5.6 まとめ

# 6. 統計的機械学習

## 6.1 k近傍法
### 簡単な例：ローンの返済不能を予測する
### 距離指標
### one-hotエンコーダ
### 標準化(正規化、z値)
### kの選択
### 特徴量エンジンとしてのk近傍法

## 6.2 木モデル
### 簡単な例
### 再帰分割アルゴリズム
### 同質性または不純度の測定
### 木の成長を止める
### 連続値を予測する
### 木の使い方

## 6.3 バギングとランダムフォレスト
### バギング
### ランダムフォレスト
### 変数の重要度

# 7. 教師なし学習


