+++
date = "2017-06-25T10:33:18+09:00"
title = "Courseraの機械学習 第1週"

categories = ["engineer"]
tags = ["機械学習"]
draft=true
+++

# 目次
* 機械学習とは
* 教師あり学習(Supervised Learning)と教師なし学習(Unsupervised Learning)
* 回帰問題と分類問題
* 線形回帰
* 目的関数
* 最急降下法
* 線形回帰における最急降下法
* 線形代数
    * 行列演算

## 機械学習とは
機械学習
Coursera: Machine Learning

### 定義：
コンピュータに明示的にプログラムすることなく学習する能力を与える研究分野 Arthur Samuel (1959)
コンピュータ・プログラムは、ある課題Tについて、ある性能基準Pに基づき、もしTについての性能が基準Pで測定して、経験Eとともに改善している場合に、経験Eから学習したと言うことができる。Tom Mitchell (1998)

## 教師ありと教師なし学習

* 教師あり(Supervised learning): コンピュータに教える
* 教師なし(Unsupervised learning): コンピュータが自分で学ぶ

現実問題に適応していく手法を学んでいく

### 教師あり学習

入力値と出力値の関係性を調べる
定義：正しい答えが与えられる（ラベル付け）
    
例えば、腫瘍が陽性か、陰性かなど。この腫瘍は陰性・陽性などといった答えが事前にされている

属性の数＝次元
SVM 無限の属性（特徴）に対応できる

### 教師無し学習

ラベルがない。もしくは全て同じ
クラスターに分けたりする
クラスタリングアルゴリズム
例）Google News：ニュースを集めそれらをグルーピングしている。クラスタリングしてまとめた
事前にデータセットをラベリングして、クラスタに分けたりしていない＝教師無し

与えられたでータセットから構造を見つけ出す。予測結果に基づくフィードバックはない

応用例：
    
* コンピュータ・クラスタリング。
* ソーシャルネットワーク分析
* マーケットセグメンテーション
* 天文学のデータ分析
* カクテル・パーティアルゴリズム

Octaveで実装していく
試作・学習用
すぐ実装できるので、これでプロトタイプを作って、移植する。
svd = 特異値分解（singular value decomposition）

シンボル
m = 訓練データのサンプル数
x = 入力値。入力変数。特徴
y = 出力値。予測する値 。目標変数

(x, y) = 一見の訓練サンプル。

X = 入力値の空間
Y = 出力値の空間
h : X -> Y

## 回帰問題と分類問題

回帰問題：継続的に出力される値を予測する
回帰（Regression）：連続値的な値を予測しようとしている
連続値出力の予測
    入力値を連続的な関数にマッピングして答えを予測する
    ある入力値が、「この値以上か以下か」みたいにすると分類問題として扱うこともできる


分類問題：予測しようとしているのが離散値。どのグループに分類されるか。
離散値出力の予測
    入力値を離散的なカテゴリーに分類する

## 線形回帰
Training Set -> Learning Algorithm -> h
h : hypothesis (仮説)　あんまり呼び名に意味はないw
    hは関数

例）家のサイズから売却価格を知る
size -> h -> price
x            y

h maps from x's to y's 
hはxからyに対応付けする関数

どのようにこの仮説hを表現するか？
ここではこのように定義
hθ(x) = θ0 + θ1x
 y = ax + b とほぼ一緒ぽい

このように線形に仮説hを定義するモデルは「線形回帰」という
変数が1つの場合は線形回帰。その変数はx
変数xで価格を予測する
単回帰ともいう

## 目的関数
Cost Function

```
hθ(x) = θ0 + θ1x
```

θi's: Parameter

どのようにデータに対してθ0とθ1のパラメータを決定していくか


これはつまり、最小化問題を解くこと

```
minimize 
θ0 θ1
```

(h(x) - y)^2 の値をできるだけ小さくするようにしたい。

```
         m
1/2m Σ (h(x) - y)^2
        i=1
```
1/2mとするのはその方が計算が楽になるかららしい

このように、目的のパラメータを求めるための式を目的関数という
（=仮説が正しいか測定するために使われる）
この目的関数は二乗誤差関数、二乗誤差目的関数と呼ばれる（最小二乗法）

一般的に、線形回帰問題でうまく機能する

このθ0とθ1は図示することで、目的関数のグラフから最小値を見ることができる
が、それを自動的に求めるアルゴリズムが必要。

## 最急降下法
目的関数Jを最小化するために使えるアルゴリズム
線形回帰以外でも用いられる手法

説明として、任意の関数Jで説明をし、その後線形回帰で用いた目的関数Jに適用する

J(θ0, θ1)

min J(θ0, θ1)
θ0,θ1

θは任意の数取ることができる（θ0 ~ θn）

概要
θ0, θ1を任意の値で初期化する(だいたいθ0=0, θ1=0で初期化する)

θ0, θ1の値を少しずつ変え続けて、J(θ0, θ1)を減少させられないか試す
→すると、最小値もしくは局所最小値（極小値）にたどり着く

初期値が変わると別の局所最適値にたどり着く特性がある。

```
repeat until convergence {
    θj := θj - α・∂/∂θj(J(θ0, θ1))
}
```

パラメータ
α：ステップの大きさ
∂/∂θj(J(θ0, θ1))：導関数。その場所における傾き。

導関数項が0 = 傾きが0の場合は最小値。収束している
導関数項が小さいほど（局所的最小値に近いほど、ステップの幅が小さくなる）
    そのため収束する
αが固定されていても徐々に収束するのはこの性質のため

αが小さいと収束までに多くのステップを要する
αが大きいと発散する恐れがある。


## 線形回帰の目的関数における最急降下法


## 行列演算

加算（減算）、スカラー積（商）、行列とベクトルの積、行列同士の積
線形回帰問題への適応、可換性、結合特性、単位行列、逆行列、転置行列


